{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "# From PDFInterpreter import both PDFResourceManager and PDFPageInterpreter\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfdevice import PDFDevice\n",
    "# Import this to raise exception whenever text extraction from PDF is not allowed\n",
    "from pdfminer.pdfpage import PDFTextExtractionNotAllowed\n",
    "from pdfminer.layout import LAParams, LTTextBox, LTTextLine\n",
    "from pdfminer.converter import PDFPageAggregator\n",
    "\n",
    "import re\n",
    "from gensim import models, corpora\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' This is what we are trying to do:\n",
    "1) Transfer information from PDF file to PDF document object. This is done using parser\n",
    "2) Get a list of PDF files (papers) from the Papers directory\n",
    "3) For each PDF file, open and parse the file using PDFParser object\n",
    "4) Assign the parsed content to PDFDocument object\n",
    "5) Now the information in this PDFDocumet object has to be processed. For this we need\n",
    "   PDFPageInterpreter, PDFDevice and PDFResourceManager\n",
    "6) Finally process the file page by page \n",
    "'''\n",
    "\n",
    "base_path = \"./Papers\"\n",
    " \n",
    "my_files = [f for f in os.listdir(base_path)]\n",
    "print (my_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = []\n",
    "papers_title = []\n",
    "\n",
    "for f in my_files:\n",
    "    password = \"\"\n",
    "    extracted_text = \"\"    \n",
    "    papers_title.append(f.split('.pdf', 1)[0].replace(' ','_'))\n",
    "    \n",
    "    fp = open(os.path.join(base_path, f), \"rb\")    \n",
    "    parser = PDFParser(fp)    \n",
    "    document = PDFDocument(parser, password)\n",
    "    if not document.is_extractable:\n",
    "        raise PDFTextExtractionNotAllowed    \n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    laparams = LAParams()\n",
    "    device = PDFPageAggregator(rsrcmgr, laparams=laparams)\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "\n",
    "    # Ok now that we have everything to process a pdf document, lets process it page by page\n",
    "    for page in PDFPage.create_pages(document):\n",
    "        # As the interpreter processes the page stored in PDFDocument object\n",
    "        interpreter.process_page(page)\n",
    "        # The device renders the layout from interpreter\n",
    "        layout = device.get_result()\n",
    "        # Out of the many LT objects within layout, we are interested in LTTextBox and LTTextLine\n",
    "        for lt_obj in layout:\n",
    "            if isinstance(lt_obj, LTTextBox) or isinstance(lt_obj, LTTextLine):\n",
    "                extracted_text += lt_obj.get_text()\n",
    "    fp.close()\n",
    "    \n",
    "    # Some preliminary data cleaning\n",
    "    extracted_text = extracted_text.lower()\n",
    "    headless_text = extracted_text.split(\"abstract\", 1)[1].split('introduction', 1)[1]\n",
    "    final_text = headless_text.rsplit('references', 1)[0].replace('-\\n','').replace('\\n', ' ').replace(ur'\\u201c', ' ').replace(ur'\\u201d', ' ').replace(ur'\\ufb01', 'fi').replace(ur'\\u2022', ' ').replace(ur'\\u2013', ' ')\n",
    "    \n",
    "    papers.append(final_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPICS = 10\n",
    "STOPWORDS = stopwords.words('english')\n",
    "COMMONWORDS = ['cid:31','figure','two','cid:27', \\\n",
    "               'cid:29','also','cid:28','cid:30', \\\n",
    "               'cid:25','cid:26','performance','data', \\\n",
    "               'cid:24','cid:21','cid:22','cid:23', 'cid:20']\n",
    "\n",
    "def clean_text(text):    \n",
    "    tokenized_text = word_tokenize(text.lower())\n",
    "    cleaned_text = [t for t in tokenized_text if t not in STOPWORDS and t not in COMMONWORDS and re.match('[a-zA-Z\\-][a-zA-Z\\-]{2,}', t)]\n",
    "    return cleaned_text\n",
    " \n",
    "# For gensim we need to tokenize the data and filter out stopwords\n",
    "tokenized_data = []\n",
    "for text in papers:\n",
    "    tokenized_data.append(clean_text(text)) \n",
    "\n",
    "# Build a Dictionary - association word to numeric id\n",
    "dictionary = corpora.Dictionary(tokenized_data)\n",
    " \n",
    "# Transform the collection of texts to a numerical form\n",
    "corpus = [dictionary.doc2bow(text) for text in tokenized_data]\n",
    "\n",
    "# Build the LDA model\n",
    "lda_model = models.LdaModel(corpus=corpus, num_topics=NUM_TOPICS, id2word=dictionary, iterations = 2000)\n",
    "\n",
    "print(\"LDA Model:\")\n",
    " \n",
    "for idx in range(NUM_TOPICS):\n",
    "    # print(\"Topic #%s:\" % idx, lda_model.print_topic(idx, 25))\n",
    "    # print without probability\n",
    "    tmp_topics = ''\n",
    "    for term in lda_model.get_topic_terms(idx, 25):\n",
    "        tmp_topics += dictionary.get(term[0]) + '; '\n",
    "    print('Topic #%s:' % idx, tmp_topics)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_idx = 0\n",
    "for text in papers:\n",
    "    print('Topics for paper %s: ' % papers_title[title_idx], lda_model.get_document_topics(dictionary.doc2bow(clean_text(text))))\n",
    "    title_idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights:\n",
    "\n",
    "- This allows us to quickly examine papers without expending human resources\n",
    "- Additional analysis are needed to determine an ideal number of topics\n",
    "    - Can we use aggregated keyword counts to inform this number?\n",
    "- Exploratory analysis with only 10 topics and 16 papers seems to be encouraging. For example, \n",
    "Topic 8 seems to focus on security subjects, and is relevant to only two papers (Hammer Attacks and SEINA) that specifically study security.\n",
    "\n",
    "### Notes:\n",
    "\n",
    "- Some PDFs are printout from HTML screen rather than direct download. This might impact the quality of pdfminer\n",
    "- More papers are needed\n",
    "\n",
    "\n",
    "### Follow-up:\n",
    "- Explore gensim to develop bigram and trigram vocabulary instead of single-word tokens. For example, \"volatile memory\" is more meaningful than \"volatile\" and \"memory\". \n",
    "- Explore parallelism in parsing PDFs to texts. \n",
    "- Explore PDFMiner and see if PDFs schematic can be extracted for localized topic models on sections such as introduction only or conclusion only. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes April 10, 2018\n",
    "\n",
    "- Topics being pursued by CloudLab users\n",
    "- Visualize topics\n",
    "- Identify pocket of researchers working on the same topic\n",
    "\n",
    "- Expanding number of topics\n",
    "- Venues of publication\n",
    "\n",
    "- Mine Google Groups: https://gist.github.com/punchagan/7947337\n",
    "\n",
    "- For a given project that uses CloudLab, which papers belong to that project. Can it be automated?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
