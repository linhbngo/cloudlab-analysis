{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Important classes to remember\\nPDFParser - fetches data from pdf file\\nPDFDocument - stores data parsed by PDFParser\\nPDFPageInterpreter - processes page contents from PDFDocument\\nPDFDevice - translates processed information from PDFPageInterpreter to whatever you need\\nPDFResourceManager - Stores shared resources such as fonts or images used by both PDFPageInterpreter and PDFDevice\\nLAParams - A layout analyzer returns a LTPage object for each page in the PDF document\\nPDFPageAggregator - Extract the decive to page aggregator to get LT object elements\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pdfTextMiner.py\n",
    "# Python 3.6.+\n",
    "# For Python 3.x use pdfminer3k module\n",
    "# This link has useful information on components of the program\n",
    "# https://euske.github.io/pdfminer/programming.html\n",
    "# http://denis.papathanasiou.org/posts/2010.08.04.post.html\n",
    "\n",
    "\n",
    "''' Important classes to remember\n",
    "PDFParser - fetches data from pdf file\n",
    "PDFDocument - stores data parsed by PDFParser\n",
    "PDFPageInterpreter - processes page contents from PDFDocument\n",
    "PDFDevice - translates processed information from PDFPageInterpreter to whatever you need\n",
    "PDFResourceManager - Stores shared resources such as fonts or images used by both PDFPageInterpreter and PDFDevice\n",
    "LAParams - A layout analyzer returns a LTPage object for each page in the PDF document\n",
    "PDFPageAggregator - Extract the decive to page aggregator to get LT object elements\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\CloudLab-36\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pdfminer.pdfparser import PDFParser, PDFDocument, PDFPage\n",
    "# From PDFInterpreter import both PDFResourceManager and PDFPageInterpreter\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter, PDFTextExtractionNotAllowed\n",
    "from pdfminer.pdfdevice import PDFDevice\n",
    "# Import this to raise exception whenever text extraction from PDF is not allowed\n",
    "from pdfminer.layout import LAParams, LTTextBox, LTTextLine\n",
    "from pdfminer.converter import PDFPageAggregator\n",
    "\n",
    "import re\n",
    "from gensim import models, corpora\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Active Learning in Performance Analysis.pdf', 'An information infrastructure framework for smart grids leveraging SDN and cloud.pdf', 'ARM Virtualization Performance and Architectural Implications.pdf', 'Brados Declarative Programmable Object Storage.pdf', 'CQSTR - Securing Cross\\xacTenant Applications with Cloud Containers.pdf', 'High-Performance ACID via Modular Concurrency Control.pdf', 'JetStream ClusterScale Parallelization of Information Flow Queries.pdf', 'One Bit Flips One Cloud Flops Cross VM Row Hammer Attacks and Privilege Escalation.pdf', 'Paving the Way for NFV Simplifying Middlebox Modifications using StateAlyzr.pdf', 'Reproducible Scientific Computing Environment with Overlay Cloud Architecture - IEEE Conference Publication.pdf', 'SEINA - A Stealthy and Effective Internal Attack in Hadoop Systems.pdf', 'Self-configuring Software-defined Overlay Bypass for Seamless Inter and Intra-cloud Virtual Networking.pdf', 'Split-level IO scheduling.pdf', 'Subways - A Case for Redundant, Inexpensive Data Center Edge Links.pdf', 'To Copy or Not To Copy- Making In-Memory Databases Fast on Modern NICs.pdf', 'To Join or Not to Join Thinking Twice about Joins before Feature Selection.pdf']\n"
     ]
    }
   ],
   "source": [
    "''' This is what we are trying to do:\n",
    "1) Transfer information from PDF file to PDF document object. This is done using parser\n",
    "2) Get a list of PDF files (papers) from the Papers directory\n",
    "3) For each PDF file, open and parse the file using PDFParser object\n",
    "4) Assign the parsed content to PDFDocument object\n",
    "5) Now the information in this PDFDocumet object has to be processed. For this we need\n",
    "   PDFPageInterpreter, PDFDevice and PDFResourceManager\n",
    "6) Finally process the file page by page \n",
    "'''\n",
    "\n",
    "base_path = \"./Papers\"\n",
    " \n",
    "my_files = [f for f in os.listdir(base_path)]\n",
    "print (my_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = []\n",
    "papers_title = []\n",
    "\n",
    "for f in my_files:\n",
    "    password = \"\"\n",
    "    extracted_text = \"\"    \n",
    "    papers_title.append(f.split('.pdf', 1)[0].replace(' ','_'))\n",
    "    \n",
    "    fp = open(os.path.join(base_path, f), \"rb\")    \n",
    "    parser = PDFParser(fp)    \n",
    "    document = PDFDocument(parser, password)\n",
    "    if not document.is_extractable:\n",
    "        raise PDFTextExtractionNotAllowed    \n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    laparams = LAParams()\n",
    "    device = PDFPageAggregator(rsrcmgr, laparams=laparams)\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "\n",
    "    # Ok now that we have everything to process a pdf document, lets process it page by page\n",
    "    for page in PDFPage.create_pages(document):\n",
    "        # As the interpreter processes the page stored in PDFDocument object\n",
    "        interpreter.process_page(page)\n",
    "        # The device renders the layout from interpreter\n",
    "        layout = device.get_result()\n",
    "        # Out of the many LT objects within layout, we are interested in LTTextBox and LTTextLine\n",
    "        for lt_obj in layout:\n",
    "            if isinstance(lt_obj, LTTextBox) or isinstance(lt_obj, LTTextLine):\n",
    "                extracted_text += lt_obj.get_text()\n",
    "    fp.close()\n",
    "    \n",
    "    # Some preliminary data cleaning\n",
    "    extracted_text = extracted_text.lower()\n",
    "    headless_text = extracted_text.split(\"abstract\", 1)[1].split('introduction', 1)[1]\n",
    "    final_text = headless_text.rsplit('references', 1)[0].replace('-\\n','').replace('\\n', ' ').replace(ur'\\u201c', ' ').replace(ur'\\u201d', ' ').replace(ur'\\ufb01', 'fi').replace(ur'\\u2022', ' ').replace(ur'\\u2013', ' ')\n",
    "    \n",
    "    papers.append(final_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Model:\n",
      "('Topic #0:', u'cloud; network; container; storage; system; using; use; containers; design; service; time; access; state; used; memory; virtual; set; log; overlay; may; computing; work; one; systems; records; ')\n",
      "('Topic #1:', u'dift; epoch; global; state; code; time; query; packet; container; local; source; service; merge; set; variables; analysis; jetstream; execution; output; cloud; log; access; processing; middlebox; cqstr; ')\n",
      "('Topic #2:', u'state; packet; service; transactions; variables; middlebox; analysis; access; callas; processing; control; transaction; output; runtime; use; group; concurrency; container; must; middleboxes; procedure; cloud; variable; example; code; ')\n",
      "('Topic #3:', u'feature; error; using; records; selection; features; transaction; analysis; set; number; transactions; use; results; ror; join; work; thus; nic; one; even; joins; section; avoid; first; used; ')\n",
      "('Topic #4:', u'epoch; global; dift; local; query; jetstream; time; source; merge; first; sources; pass; epochs; set; execution; sinks; sink; taint; analysis; log; aggregation; identifier; instrumentation; node; cores; ')\n",
      "('Topic #5:', u'arm; tor; type; hypervisor; xen; hardware; servers; tors; server; network; kvm; rack; load; switches; subways; traffic; using; physical; virtualization; racks; virtual; switch; support; cluster; balancing; ')\n",
      "('Topic #6:', u'arm; state; hypervisor; xen; packet; variables; virtual; kvm; cloud; memory; hardware; type; computing; virtualization; using; use; used; application; analysis; run; processing; support; table; software; variable; ')\n",
      "('Topic #7:', u'overlay; network; vias; cloud; sdn; virtual; service; address; nested; virtualization; across; controller; bypass; infrastructure; information; resources; instances; set; container; providers; grid; within; vms; nat; power; ')\n",
      "('Topic #8:', u'memory; row; attacks; task; hammer; attack; execution; bits; time; bit; system; page; one; tasks; node; machine; physical; first; hadoop; dram; set; may; address; systems; bank; ')\n",
      "('Topic #9:', u'transactions; row; callas; transaction; memory; one; server; servers; bit; type; first; tors; physical; using; table; cluster; attacks; concurrency; group; example; tor; within; single; different; however; ')\n"
     ]
    }
   ],
   "source": [
    "NUM_TOPICS = 10\n",
    "STOPWORDS = stopwords.words('english')\n",
    "COMMONWORDS = ['cid:31','figure','two','cid:27', \\\n",
    "               'cid:29','also','cid:28','cid:30', \\\n",
    "               'cid:25','cid:26','performance','data', \\\n",
    "               'cid:24','cid:21','cid:22','cid:23', 'cid:20']\n",
    "\n",
    "def clean_text(text):    \n",
    "    tokenized_text = word_tokenize(text.lower())\n",
    "    cleaned_text = [t for t in tokenized_text if t not in STOPWORDS and t not in COMMONWORDS and re.match('[a-zA-Z\\-][a-zA-Z\\-]{2,}', t)]\n",
    "    return cleaned_text\n",
    " \n",
    "# For gensim we need to tokenize the data and filter out stopwords\n",
    "tokenized_data = []\n",
    "for text in papers:\n",
    "    tokenized_data.append(clean_text(text)) \n",
    "\n",
    "# Build a Dictionary - association word to numeric id\n",
    "dictionary = corpora.Dictionary(tokenized_data)\n",
    " \n",
    "# Transform the collection of texts to a numerical form\n",
    "corpus = [dictionary.doc2bow(text) for text in tokenized_data]\n",
    "\n",
    "# Build the LDA model\n",
    "lda_model = models.LdaModel(corpus=corpus, num_topics=NUM_TOPICS, id2word=dictionary, iterations = 2000)\n",
    "\n",
    "print(\"LDA Model:\")\n",
    " \n",
    "for idx in range(NUM_TOPICS):\n",
    "    # print(\"Topic #%s:\" % idx, lda_model.print_topic(idx, 25))\n",
    "    # print without probability\n",
    "    tmp_topics = ''\n",
    "    for term in lda_model.get_topic_terms(idx, 25):\n",
    "        tmp_topics += dictionary.get(term[0]) + '; '\n",
    "    print('Topic #%s:' % idx, tmp_topics)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Topics for paper Active_Learning_in_Performance_Analysis: ', [(3, 0.903128), (4, 0.09662951)])\n",
      "('Topics for paper An_information_infrastructure_framework_for_smart_grids_leveraging_SDN_and_cloud: ', [(0, 0.106043525), (7, 0.89361215)])\n",
      "('Topics for paper ARM_Virtualization_Performance_and_Architectural_Implications: ', [(5, 0.77718997), (6, 0.22267243)])\n",
      "('Topics for paper Brados_Declarative_Programmable_Object_Storage: ', [(0, 0.9997882)])\n",
      "('Topics for paper CQSTR_-_Securing_Cross\\xacTenant_Applications_with_Cloud_Containers: ', [(0, 0.7687041), (1, 0.23114716)])\n",
      "('Topics for paper High-Performance_ACID_via_Modular_Concurrency_Control: ', [(9, 0.99985105)])\n",
      "('Topics for paper JetStream_ClusterScale_Parallelization_of_Information_Flow_Queries: ', [(4, 0.999839)])\n",
      "('Topics for paper One_Bit_Flips_One_Cloud_Flops_Cross_VM_Row_Hammer_Attacks_and_Privilege_Escalation: ', [(8, 0.9998557)])\n",
      "('Topics for paper Paving_the_Way_for_NFV_Simplifying_Middlebox_Modifications_using_StateAlyzr: ', [(1, 0.032809928), (2, 0.38982838), (6, 0.577248)])\n",
      "('Topics for paper Reproducible_Scientific_Computing_Environment_with_Overlay_Cloud_Architecture_-_IEEE_Conference_Publication: ', [(0, 0.5554026), (6, 0.44432417)])\n",
      "('Topics for paper SEINA_-_A_Stealthy_and_Effective_Internal_Attack_in_Hadoop_Systems: ', [(8, 0.9997073)])\n",
      "('Topics for paper Self-configuring_Software-defined_Overlay_Bypass_for_Seamless_Inter_and_Intra-cloud_Virtual_Networking: ', [(7, 0.9998269)])\n",
      "('Topics for paper Split-level_IO_scheduling: ', [(0, 0.6534541), (8, 0.29643983), (9, 0.04998342)])\n",
      "('Topics for paper Subways_-_A_Case_for_Redundant,_Inexpensive_Data_Center_Edge_Links: ', [(5, 0.99984366)])\n",
      "('Topics for paper To_Copy_or_Not_To_Copy-_Making_In-Memory_Databases_Fast_on_Modern_NICs: ', [(0, 0.10862693), (3, 0.89113563)])\n",
      "('Topics for paper To_Join_or_Not_to_Join_Thinking_Twice_about_Joins_before_Feature_Selection: ', [(3, 0.999843)])\n"
     ]
    }
   ],
   "source": [
    "title_idx = 0\n",
    "for text in papers:\n",
    "    print('Topics for paper %s: ' % papers_title[title_idx], lda_model.get_document_topics(dictionary.doc2bow(clean_text(text))))\n",
    "    title_idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights:\n",
    "\n",
    "- This allows us to quickly examine papers without expending human resources\n",
    "- Additional analysis are needed to determine an ideal number of topics\n",
    "    - Can we use aggregated keyword counts to inform this number?\n",
    "- Exploratory analysis with only 10 topics and 16 papers seems to be encouraging. For example, \n",
    "Topic 8 seems to focus on security subjects, and is relevant to only two papers (Hammer Attacks and SEINA) that specifically study security.\n",
    "\n",
    "### Notes:\n",
    "\n",
    "- Some PDFs are printout from HTML screen rather than direct download. This might impact the quality of pdfminer\n",
    "- More papers are needed\n",
    "\n",
    "\n",
    "### Follow-up:\n",
    "- Explore gensim to develop bigram and trigram vocabulary instead of single-word tokens. For example, \"volatile memory\" is more meaningful than \"volatile\" and \"memory\". \n",
    "- Explore parallelism in parsing PDFs to texts. \n",
    "- Explore PDFMiner and see if PDFs schematic can be extracted for localized topic models on sections such as introduction only or conclusion only. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes April 10, 2018\n",
    "\n",
    "- Topics being pursued by CloudLab users\n",
    "- Visualize topics\n",
    "- Identify pocket of researchers working on the same topic\n",
    "\n",
    "- Expanding number of topics\n",
    "- Venues of publication\n",
    "\n",
    "- Mine Google Groups: https://gist.github.com/punchagan/7947337\n",
    "\n",
    "- For a given project that uses CloudLab, which papers belong to that project. Can it be automated?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CloudLab-36",
   "language": "python",
   "name": "cloudlab-36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
